项目：baidunews 问题：
1.在运行爬虫几次后，就再也访问不了了，推测是服务器端做了限制；
2018-03-24 17:11:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://163.com/robots.txt> (failed 2 times): Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.
2018-03-24 17:11:44 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://163.com/robots.txt> (failed 3 times): Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.
2018-03-24 17:11:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://163.com/robots.txt>: Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.
2018-03-24 17:11:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://163.com/> (failed 1 times): Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.
2018-03-24 17:11:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://163.com/> (failed 2 times): Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.
2018-03-24 17:11:56 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://163.com/> (failed 3 times): Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.
2018-03-24 17:11:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://163.com/>: Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.

解决方法参考：https://blog.csdn.net/haipengdai/article/details/48545231

运行时遇到的新的问题解决请参考：
https://blog.csdn.net/x631617479/article/details/72519086

需要把上面的setttings中的 example -->baidunews，这个故障改善了。
但继续测试时，还是报相同的错误，即服务器拒绝访问。
==》最终打开fiddler软件后，故障消失了，好坑呀，早知道早点打开fiddler了，原因是什么待后续继续确认？？？？？？


2.

在网页多层的情况下，只执行了1层。第二层的yield Requset不执行
问题描述：在n1爬虫中， 第一层next中的reauest可以执行，第二层next2中的request一直没有返回
解决办法：
追加参数：dont_filter=True

before: Request(thisurl,callback=self.next2)
after:  Request(thisurl,callback=self.next2,dont_filter=True)

原因解释：
https://blog.csdn.net/honglicu123/article/details/75453107
信息摘录如下：
在 scrapy 中，
scrapy.Request(url, headers=self.header, callback=self.parse_detail)
调试的时候，发现回调函数 parse_detail 没有被调用，这可能就是被过滤掉了，查看 scrapy 的输出日志 offsite/filtered 会显示过滤的数目。这个问题如何解决呢，查看手册发现(https://doc.scrapy.org/en/latest/faq.html?highlight=offsite%2Ffiltered)这个问题，这些日志信息都是由 scrapy 中的一个 middleware 抛出的，如果没有自定义，那么这个 middleware 就是默认的 Offsite Spider Middleware，它的目的就是过滤掉那些不在 allowed_domains 列表中的请求 requests。

再次查看手册中关于 OffsiteMiddleware 的部分(https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware)
两种方法能够使 requests 不被过滤:
1. 在 allowed_domains 中加入 url
2. 在 scrapy.Request() 函数中将参数 dont_filter=True 设置为 True

如下摘自手册

If the spider doesn’t define an allowed_domains attribute, or the attribute is empty, the offsite middleware will allow all requests.

If the request has the dont_filter attribute set, the offsite middleware will allow the request even if its domain is not listed in allowed domains.

3.douban项目， 爬取网页时提示：
2018-03-25 20:37:09 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "accounts.douban.com"; '*.douban.com'!='accounts.douban.com'
2018-03-25 20:37:09 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://accounts.douban.com/login> (referer: None)
2018-03-25 20:37:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://accounts.douban.com/login>: HTTP status code is not handled or not allowed

类似的错误请参考:https://blog.csdn.net/iefreer/article/details/34631291

问题
抓取数据时，通常调试信息是：

DEBUG: Crawled (200) <GET http://www.techbrood.com/> (referer: None)

如果出现

DEBUG: Crawled (403) <GET http://www.techbrood.com/> (referer: None)

表示网站采用了防爬技术anti-web-crawling technique（Amazon所用），比较简单即会检查用户代理（User Agent）信息。


解决方法
在请求头部构造一个User Agent，如下所示：

def start_requests(self):
    yield Request("http://www.techbrood.com/",
                  headers={'User-Agent': "your agent string"})


