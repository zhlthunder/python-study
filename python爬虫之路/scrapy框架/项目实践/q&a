��Ŀ��baidunews ���⣺
1.���������漸�κ󣬾���Ҳ���ʲ����ˣ��Ʋ��Ƿ��������������ƣ�
2018-03-24 17:11:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://163.com/robots.txt> (failed 2 times): Connection was refused by other side: 10061: ����Ŀ�����������ܾ����޷����ӡ�.
2018-03-24 17:11:44 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://163.com/robots.txt> (failed 3 times): Connection was refused by other side: 10061: ����Ŀ�����������ܾ����޷����ӡ�.
2018-03-24 17:11:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://163.com/robots.txt>: Connection was refused by other side: 10061: ����Ŀ�����������ܾ����޷����ӡ�.
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: ����Ŀ�����������ܾ����޷����ӡ�.
2018-03-24 17:11:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://163.com/> (failed 1 times): Connection was refused by other side: 10061: ����Ŀ�����������ܾ����޷����ӡ�.
2018-03-24 17:11:55 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://163.com/> (failed 2 times): Connection was refused by other side: 10061: ����Ŀ�����������ܾ����޷����ӡ�.
2018-03-24 17:11:56 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://163.com/> (failed 3 times): Connection was refused by other side: 10061: ����Ŀ�����������ܾ����޷����ӡ�.
2018-03-24 17:11:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://163.com/>: Connection was refused by other side: 10061: ����Ŀ�����������ܾ����޷����ӡ�.

��������ο���https://blog.csdn.net/haipengdai/article/details/48545231

����ʱ�������µ���������ο���
https://blog.csdn.net/x631617479/article/details/72519086

��Ҫ�������setttings�е� example -->baidunews��������ϸ����ˡ�
����������ʱ�����Ǳ���ͬ�Ĵ��󣬼��������ܾ����ʡ�
==�����մ�fiddler����󣬹�����ʧ�ˣ��ÿ�ѽ����֪������fiddler�ˣ�ԭ����ʲô����������ȷ�ϣ�����������


2.

����ҳ��������£�ִֻ����1�㡣�ڶ����yield Requset��ִ��
������������n1�����У� ��һ��next�е�reauest����ִ�У��ڶ���next2�е�requestһֱû�з���
����취��
׷�Ӳ�����dont_filter=True

before: Request(thisurl,callback=self.next2)
after:  Request(thisurl,callback=self.next2,dont_filter=True)

ԭ����ͣ�
https://blog.csdn.net/honglicu123/article/details/75453107
��Ϣժ¼���£�
�� scrapy �У�
scrapy.Request(url, headers=self.header, callback=self.parse_detail)
���Ե�ʱ�򣬷��ֻص����� parse_detail û�б����ã�����ܾ��Ǳ����˵��ˣ��鿴 scrapy �������־ offsite/filtered ����ʾ���˵���Ŀ�����������ν���أ��鿴�ֲᷢ��(https://doc.scrapy.org/en/latest/faq.html?highlight=offsite%2Ffiltered)������⣬��Щ��־��Ϣ������ scrapy �е�һ�� middleware �׳��ģ����û���Զ��壬��ô��� middleware ����Ĭ�ϵ� Offsite Spider Middleware������Ŀ�ľ��ǹ��˵���Щ���� allowed_domains �б��е����� requests��

�ٴβ鿴�ֲ��й��� OffsiteMiddleware �Ĳ���(https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware)
���ַ����ܹ�ʹ requests ��������:
1. �� allowed_domains �м��� url
2. �� scrapy.Request() �����н����� dont_filter=True ����Ϊ True

����ժ���ֲ�

If the spider doesn��t define an allowed_domains attribute, or the attribute is empty, the offsite middleware will allow all requests.

If the request has the dont_filter attribute set, the offsite middleware will allow the request even if its domain is not listed in allowed domains.

3.douban��Ŀ�� ��ȡ��ҳʱ��ʾ��
2018-03-25 20:37:09 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for hostname "accounts.douban.com"; '*.douban.com'!='accounts.douban.com'
2018-03-25 20:37:09 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://accounts.douban.com/login> (referer: None)
2018-03-25 20:37:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://accounts.douban.com/login>: HTTP status code is not handled or not allowed

���ƵĴ�����ο�:https://blog.csdn.net/iefreer/article/details/34631291

����
ץȡ����ʱ��ͨ��������Ϣ�ǣ�

DEBUG: Crawled (200) <GET http://www.techbrood.com/> (referer: None)

�������

DEBUG: Crawled (403) <GET http://www.techbrood.com/> (referer: None)

��ʾ��վ�����˷�������anti-web-crawling technique��Amazon���ã����Ƚϼ򵥼������û�����User Agent����Ϣ��


�������
������ͷ������һ��User Agent��������ʾ��

def start_requests(self):
    yield Request("http://www.techbrood.com/",
                  headers={'User-Agent': "your agent string"})




3.scrapy使用时，settings.py中需要配置的部分包括：
##配置全局的UA
USER_AGENT = 'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6'

ROBOTSTXT_OBEY = False  //从True 改为False


4. 开启fiddler会导致爬虫运行识别，可以关注一下，如果运行很慢，可以把fiddler关闭后再试试看。
